proportion<-c(proportion, x)
}
Genotype1<-cg.high[,1]
Genotype2<-cg.high[,2]
cg.high<-cbind(Genotype1, Genotype2, proportion)
write.csv(file="7_Unusually similiar genotypes.csv", cg.high)
##########################################################################################
x<-(nind(cross))
x<-x*0.15
missing.ind<-nmissing(cross, what="mar")
missing.ind<-missing.ind[missing.ind > x]
nmi<-"Number of Individuals without marker data"
missing.ind<-c(nmi, missing.ind)
write.csv(file="8_Poorly typed markers.csv", missing.ind)
##########################################################################################
x<-(totmar(cross))
x<-x*0.15
number.missing.mar<-nmissing(cross, what="ind")
Line.ID<-1:nind(cross)
missing.mar<-cbind(Line.ID, number.missing.mar)
missing.mar<-missing.mar[missing.mar[,2]>x,]
write.csv(file="9_Poorly typed individuals.csv", missing.mar)
##########################################################################################
cross.sc1<-scanone(cross, pheno.col=1:nphe(cross), method="em", use="all.obs", model="normal")
##########################################################################################
cross.sc1.perms<-scanone(cross, pheno.col=1:nphe(cross), method="em", n.perm=1000, verbose=TRUE, model="normal", n.cluster=12)
##########################################################################################
sum<-summary(cross.sc1, threshold=0, perms=cross.sc1.perms, pvalues=TRUE, format="tabByCol", ci.function="lodint", drop=1.5, expandtomarkers=TRUE)
space<-" "
for(i in 2:nphe(cross)){
x<-capture.output(sum[[i]])
cat(colnames(cross$pheno[i]), file="10_Initial QTL hits by phenotype.txt", sep="\n", append=TRUE)
cat(x, file="10_Summary of top hits by phenotype.txt", sep="\n", append=TRUE)
cat(space, file="10_Summary of top hits by phenotype.txt", sep="\n", append=TRUE)
}
##########################################################################################
lods<-capture.output(cross.sc1)
cat(lods, file="11_LOD scores for every marker.txt", sep="\n")
##########################################################################################
z<-summary(cross.sc1.perms, alpha=.05)
pdf(file="12_QTL Plots.pdf", width=11, height=8.5)
for(i in 2:nphe(cross)){
plot(cross.sc1, lodcolumn=i, lwd=1.5, gap=0, bandcol="gray70", incl.markers=TRUE, main=colnames(cross$pheno[i]), xlab=c("Threshold for alpha=.05 using 1000 permutations", z[i]))
add.threshold(cross.sc1, perms=cross.sc1.perms, alpha=0.05, lodcolumn=i, gap=0)
}
dev.off()
##########################################################################################
newmap<-pull.map(cross)
for(i in 1:length(names(newmap))){
snps<-names(newmap[[i]])
gm<-c(snps, newmap[[i]])
gm2<-matrix(gm, ncol=2)
write.table(file="13_Genetic Map Positions.csv", sep=",", append=TRUE, gm2)
}
##########################################################################################
cross2<-sim.geno(cross)
##########################################################################################
for(k in 2:nphe(cross)){
phenotype<-k
pheno<-colnames(cross$pheno[phenotype])
##########################################################################################
#(24) Makes a qtl object containing the ALL the qtl from file="10_Initial QTL hits by phenotype"
#for the phenotype you specified.
##########################################################################################
chromo<-sum[[pheno]]
chr<-{}
pos<-{}
for(i in 1:nrow(chromo)){
chr1<-chromo[i,1]
chr2<-as.numeric(as.character(chr1))
chr<-c(chr, chr2)
}
for(i in 1:nrow(chromo)){
pos1<-chromo[i,2]
pos<-c(pos, pos1)
}
if(is.na(chr[1])==FALSE) {
qtl<-makeqtl(cross, chr=chr, pos=pos, what="prob")
createqtl<- paste("Q", 1:qtl$n.qtl, sep="")
formula<-as.formula(paste("y ~ ", paste(createqtl, collapse= "+")))
##########################################################################################
#(25) Scans for additional linked QTL conditioning on the QTL already detected. .
#*NOTE*# You may receive warning messages about dropping individuals with missing phenotype data
#and/or that the column names in scanone input do not match those in perms input. These are both
#expected under some circumstances and do not effect the output of the code. Also you may recieve
#a warning that there is no chromosome number NA if no additional QTL are to be found.
##########################################################################################
cross.aq<-addqtl(cross, pheno.col=phenotype, qtl=qtl, formula=formula, method="hk", model="normal")
sub.perms<-subset(cross.sc1.perms, lodcolumn=phenotype)
xx<-capture.output(summary(cross.aq, perms=sub.perms, alpha=.05, pvalues=TRUE, format="tabByCol", ci.function="lodint", drop=1.5, expandtomarkers=TRUE))
xxy<-c(pheno,xx)
cat(xxy, file="14_Additional QTL hits by phenotype.txt", sep="\n", append=TRUE)
cat(space, file="14_Additional QTL hits by phenotype.txt", sep="\n", append=TRUE)
##########################################################################################
#(26) Adds additional QTL (if any) in the file "14_Additional QTL hits by phenotype.txt"
#to the qtl object and update the formula object.
##########################################################################################
sum.aq<-summary(cross.aq, perms=sub.perms, alpha=.05, pvalues=TRUE, format="tabByCol", ci.function="lodint", drop=0.95, expandtomarkers=TRUE)
chr.aq<-{}
pos.aq<-{}
for(i in 1:nrow(sum.aq$lod)){
chr.aq1<-sum.aq$lod[i,1]
chr.aq2<-as.numeric(as.character(chr.aq1))
chr.aq<-c(chr.aq, chr.aq1)
}
if(is.na(chr.aq)==TRUE) { chr.aq<-chr } else { chr.aq<-c(chr, chr.aq) }
for(i in 1:nrow(sum.aq$lod)){
pos1.aq<-sum.aq$lod[i,2]
pos.aq<-c(pos.aq, pos1.aq)
}
if(is.na(pos.aq)==TRUE) { pos.aq<-pos } else { pos.aq<-c(pos,pos.aq) }
qtl<-makeqtl(cross, chr=chr.aq, pos=pos.aq, what="prob")
createqtl<- paste("Q", 1:qtl$n.qtl, sep="")
formula<-as.formula(paste("y ~ ", paste(createqtl, collapse= "+")))
##########################################################################################
#(27) Uses forward selection and backward elimination model selection to probe the model space
#for the best fit QTL model explaining your data.
##########################################################################################
pen<-summary(sub.perms)
cross.sw<-stepwiseqtl(cross, pheno.col=phenotype, qtl=qtl, formula=formula, method="hk", penalties=pen, model="normal", additive.only=TRUE)
swQTL<-capture.output(print(cross.sw))
swQTL2<-c(pheno, swQTL)
cat(swQTL2, file="15_Additional QTL hits from stepwise analysis.txt", sep="\n", append=TRUE)
cat(space, file="15_Additional QTL hits from stepwise analysis.txt", sep="\n", append=TRUE)
##########################################################################################
#(28) Adds additional QTL (if any) to the qtl object found by stepwise model selection.
##########################################################################################
sum.sw<-summary(cross.sw)
chr.sw<-{}
pos.sw<-{}
for(i in 1:nrow(sum.sw)){
chr.sw1<-sum.sw[i,2]
chr.sw2<-as.numeric(as.character(chr.sw1))
chr.sw<-c(chr.sw, chr.sw2)
}
for(i in 1:nrow(sum.sw)){
pos1.sw<-sum.sw[i,3]
pos.sw<-c(pos.sw, pos1.sw)
}
qtl2<-makeqtl(cross, chr=chr.sw, pos=pos.sw, what="prob")
createqtl<- paste("Q", 1:qtl2$n.qtl, sep="")
formula<-as.formula(paste("y ~ ", paste(createqtl, collapse= "+")))
rqtl<-refineqtl(cross, pheno.col=phenotype, qtl=qtl2, method="hk", model="normal")
##########################################################################################
#(29) Writes a file containing the peak marker and the flanking markers representing the 1.5 LOD interval of each QTL.
##########################################################################################
Q<-"Q"
space<-" "
for (i in 1:rqtl$n.qtl){
interval<-capture.output(lodint(rqtl, qtl.index=i, drop=1.5, expandtomarkers=TRUE))
q<-paste(Q, i, sep="")
interval.new<-c(pheno, q, interval, space)
cat(interval.new, file="16_Summary of Final QTL Intervals.txt", sep="\n", append=TRUE)
}
##########################################################################################
#(30) Writes a csv file containing the results of ANOVA for the full and reduced models,
#the % variance explained by each QTL and the estimated effect size
#(half the distance between the means for each genotype class in the case of RILs).
#*NOTE*# You may receive a warning here about dropping individuals with missing phenotypes.
#This is expected if such a case exists and does not effect the output of the code.
##########################################################################################
cross.ests<-fitqtl(cross, pheno.col=phenotype, qtl=rqtl, formula=formula, method="hk", dropone=TRUE, get.ests=TRUE, model="normal")
ests<-capture.output(summary(cross.ests))
ests<-c(pheno, ests)
write(ests, file="17_ANOVA results and QTL effect estimates.txt", sep="\n", append=TRUE)
##########################################################################################
#(31) Generates a pdf file of the effect plots and a text file with means and standard error for each genotype class.
#Generates a separate pdf file for every QTL.
#*NOTE*# This will likely produce a warning indicating that column names are being appended to file.
##########################################################################################
for(i in 1:length(chr.sw)) {
b<-paste("Q",i, ".pdf",sep="")
file<-paste("18_Marker effect plots", pheno, b)
mar<-find.marker(cross2, chr=chr.sw[i], pos=pos.sw[i])
pdf(file=file, width=11, height=8.5)
plot.pxg(cross, marker=mar, pheno.col=phenotype)
dev.off()
phenoqtl<-paste(pheno, b)
pheno.eff<-c(phenoqtl, space)
means<-effectplot(cross2, pheno.col=phenotype, mname1=mar, draw=FALSE)
cat(pheno.eff, file="19_means and SE.txt", sep="\n", append=TRUE)
write.table(means$Means, file="19_means and SE.txt", sep=",", col.names="Means", row.names=TRUE, append=TRUE)
cat(space, file="19_means and SE.txt", sep="\n", append=TRUE)
write.table(means$SEs, file="19_means and SE.txt", sep=",", col.names="Standard Error", row.names=TRUE, append=TRUE)
cat(space, file="19_means and SE.txt", sep="\n", append=TRUE)
}
##########################################################################################
#(32) Contingency statement if no QTL are detected in the initial genome scan for a given phenotype. You can ignore this step if you are not running the entire model selection section as a loop.
##########################################################################################
} else { null<-"	There were no LOD peaks above the threshold"
cat(pheno, file="14_Additional QTL hits by phenotype.txt", sep="\n", append=TRUE)
cat(null, file="14_Additional QTL hits by phenotype.txt", sep="\n", append=TRUE)
cat(space, file="14_Additional QTL hits by phenotype.txt", sep="\n", append=TRUE)
cat(pheno, file="15_Additional QTL hits from stepwise analysis.txt", sep="\n", append=TRUE)
cat(null, file="15_Additional QTL hits from stepwise analysis.txt", sep="\n", append=TRUE)
cat(space, file="15_Additional QTL hits from stepwise analysis.txt", sep="\n", append=TRUE)
cat(pheno, file="16_Summary of Final QTL Intervals.txt", sep="\n", append=TRUE)
cat(null, file="16_Summary of Final QTL Intervals.txt", sep="\n", append=TRUE)
cat(space, file="16_Summary of Final QTL Intervals.txt", sep="\n", append=TRUE)
cat(pheno, file="17_ANOVA results and QTL effect estimates.txt", sep="\n", append=TRUE)
cat(null, file="17_ANOVA results and QTL effect estimates.txt", sep="\n", append=TRUE)
cat(space, file="17_ANOVA results and QTL effect estimates.txt", sep="\n", append=TRUE)
cat(pheno, file="19_means and SE.txt", sep="\n", append=TRUE)
cat(null, file="19_means and SE.txt", sep="\n", append=TRUE)
cat(space, file="19_means and SE.txt", sep="\n", append=TRUE) }
}
##########################################################################################
setwd("~/Documents/Abiodun Olayinka/DATA ANALYSIS/NDVI Publication")
# Read the data
# Replace the file path with the actual path to your file
data <- read_excel("mokwapoly9.xlsx", sheet = "Sheet1")
library(readxl)
# Read the data
# Replace the file path with the actual path to your file
data <- read_excel("mokwapoly9.xlsx", sheet = "Sheet1")
# Display basic information about the dataset
cat("Dataset dimensions:", dim(data), "\n")
cat("Column names:\n")
print(names(data))
# Check for missing values in key variables
summary(data[, c("FYLD", "NDVI", "PLTHT", "CHL", "STMDI", "ANGBR", "LODG", "NOHAV", "PPSTD")])
# Remove rows with missing values in the variables of interest
# Convert the data to a data frame for easier manipulation
data_df <- as.data.frame(data)
# Select only the columns we need and remove rows with NA in critical variables
model_data <- data_df[, c("FYLD", "NDVI", "PLTHT", "CHL", "ANGBR", "STMDI", "LODG", "NOHAV", "PPSTD")]
# Check for NA values
na_count <- colSums(is.na(model_data))
cat("\nNumber of NA values per column:\n")
print(na_count)
# Remove rows with NA values
model_data <- na.omit(model_data)
cat("\nRows after removing NAs:", nrow(model_data), "\n")
# Method 1: Using I() function within lm
poly_model <- lm(FYLD ~ NDVI + I(NDVI^2) +
PLTHT + I(PLTHT^2) +
CHL + I(CHL^2) +
ANGBR + I(ANGBR^2) +
STMDI + I(STMDI^2) +
LODG + I(LODG^2) +
NOHAV + I(NOHAV^2) +
PPSTD + I(PPSTD^2),
data = model_data)
# Method 2: Using poly() function (produces orthogonal polynomials)
# This can help with multicollinearity issues
poly_model_orthogonal <- lm(FYLD ~ poly(NDVI, 2, raw = FALSE) +
poly(PLTHT, 2, raw = FALSE) +
poly(CHL, 2, raw = FALSE) +
poly(ANGBR, 2, raw = FALSE) +
poly(STMDI, 2, raw = FALSE) +
poly(LODG, 2, raw = FALSE) +
poly(NOHAV, 2, raw = FALSE) +
poly(PPSTD, 2, raw = FALSE),
data = model_data)
# Display model summary
cat("\n", paste(rep("=", 80), collapse = ""), "\n")
cat("POLYNOMIAL REGRESSION MODEL SUMMARY (with raw polynomials)\n")
cat(paste(rep("=", 80), collapse = ""), "\n")
print(summary(poly_model))
cat("\n", paste(rep("=", 80), collapse = ""), "\n")
cat("POLYNOMIAL REGRESSION MODEL SUMMARY (with orthogonal polynomials)\n")
cat(paste(rep("=", 80), collapse = ""), "\n")
print(summary(poly_model_orthogonal))
# Extract coefficients
cat("\n", paste(rep("=", 80), collapse = ""), "\n")
cat("MODEL COEFFICIENTS\n")
cat(paste(rep("=", 80), collapse = ""), "\n")
coefficients_df <- as.data.frame(coef(poly_model))
names(coefficients_df) <- "Coefficient"
print(coefficients_df)
# Calculate and display confidence intervals
cat("\n", paste(rep("=", 80), collapse = ""), "\n")
cat("CONFIDENCE INTERVALS (95%)\n")
cat(paste(rep("=", 80), collapse = ""), "\n")
conf_int <- confint(poly_model, level = 0.95)
print(conf_int)
# ANOVA table
cat("\n", paste(rep("=", 80), collapse = ""), "\n")
cat("ANOVA TABLE\n")
cat(paste(rep("=", 80), collapse = ""), "\n")
print(anova(poly_model))
# Model diagnostics
cat("\n", paste(rep("=", 80), collapse = ""), "\n")
cat("MODEL DIAGNOSTICS\n")
cat(paste(rep("=", 80), collapse = ""), "\n")
# R-squared and Adjusted R-squared
cat("R-squared:", summary(poly_model)$r.squared, "\n")
cat("Adjusted R-squared:", summary(poly_model)$adj.r.squared, "\n")
cat("F-statistic:", summary(poly_model)$fstatistic[1], "\n")
cat("p-value (F-test):", pf(summary(poly_model)$fstatistic[1],
summary(poly_model)$fstatistic[2],
summary(poly_model)$fstatistic[3],
lower.tail = FALSE), "\n")
# AIC and BIC
cat("AIC:", AIC(poly_model), "\n")
cat("BIC:", BIC(poly_model), "\n")
# Check for multicollinearity using Variance Inflation Factor (VIF)
# Install car package if needed
if (!require(car)) {
install.packages("car")
library(car)
}
cat("\n", paste(rep("=", 80), collapse = ""), "\n")
cat("VARIANCE INFLATION FACTOR (VIF) - check for multicollinearity\n")
cat(paste(rep("=", 80), collapse = ""), "\n")
tryCatch({
vif_values <- vif(poly_model)
print(vif_values)
}, error = function(e) {
cat("Could not calculate VIF due to: ", e$message, "\n")
})
# Create diagnostic plots
cat("\n", paste(rep("=", 80), collapse = ""), "\n")
cat("GENERATING DIAGNOSTIC PLOTS\n")
cat(paste(rep("=", 80), collapse = ""), "\n")
# Set up plotting area
par(mfrow = c(2, 2))
plot(poly_model)
par(mfrow = c(1, 1))
# Additional diagnostic plots
# Residuals vs Fitted with smoother
plot(poly_model$fitted.values, poly_model$residuals,
main = "Residuals vs Fitted Values",
xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
lines(lowess(poly_model$fitted.values, poly_model$residuals), col = "blue")
# Q-Q plot for normality of residuals
qqnorm(poly_model$residuals, main = "Q-Q Plot of Residuals")
qqline(poly_model$residuals, col = "red")
# Shapiro-Wilk test for normality
cat("\nShapiro-Wilk test for normality of residuals:\n")
shapiro_test <- shapiro.test(poly_model$residuals[1:5000])  # Test only first 5000 due to sample size limit
print(shapiro_test)
# Predicted vs Actual values plot
predicted <- predict(poly_model)
plot(model_data$FYLD, predicted,
main = "Predicted vs Actual FYLD",
xlab = "Actual FYLD", ylab = "Predicted FYLD")
abline(0, 1, col = "red", lty = 2)
# Calculate prediction accuracy metrics
rmse <- sqrt(mean((predicted - model_data$FYLD)^2))
mae <- mean(abs(predicted - model_data$FYLD))
cat("\nPrediction Accuracy Metrics:\n")
cat("RMSE:", rmse, "\n")
cat("MAE:", mae, "\n")
cat("Mean FYLD:", mean(model_data$FYLD), "\n")
cat("RMSE as % of mean:", (rmse/mean(model_data$FYLD))*100, "%\n")
# Create a summary table of important statistics
cat("\n", paste(rep("=", 80), collapse = ""), "\n")
cat("MODEL SUMMARY TABLE\n")
cat(paste(rep("=", 80), collapse = ""), "\n")
# Extract coefficients, standard errors, t-values, and p-values
coef_summary <- summary(poly_model)$coefficients
coef_table <- data.frame(
Estimate = coef_summary[, 1],
t_value = coef_summary[, 3],
Significance = ifelse(coef_summary[, 4] < 0.001, "***",
ifelse(coef_summary[, 4] < 0.05, "*",
)
print(coef_table)
library(readxl)
# Read the data
# Replace the file path with the actual path to your file
data <- read_excel("mokwapoly9.xlsx", sheet = "Sheet1")
# Display basic information about the dataset
cat("Dataset dimensions:", dim(data), "\n")
cat("Column names:\n")
print(names(data))
# Check for missing values in key variables
summary(data[, c("FYLD", "NDVI", "PLTHT", "CHL", "STMDI", "ANGBR", "LODG", "NOHAV", "PPSTD")])
# Remove rows with missing values in the variables of interest
# Convert the data to a data frame for easier manipulation
data_df <- as.data.frame(data)
# Select only the columns we need and remove rows with NA in critical variables
model_data <- data_df[, c("FYLD", "NDVI", "PLTHT", "CHL", "ANGBR", "STMDI", "LODG", "NOHAV", "PPSTD")]
# Check for NA values
na_count <- colSums(is.na(model_data))
cat("\nNumber of NA values per column:\n")
print(na_count)
# Remove rows with NA values
model_data <- na.omit(model_data)
cat("\nRows after removing NAs:", nrow(model_data), "\n")
# Method 1: Using I() function within lm
poly_model <- lm(FYLD ~ NDVI + I(NDVI^2) +
PLTHT + I(PLTHT^2) +
CHL + I(CHL^2) +
ANGBR + I(ANGBR^2) +
STMDI + I(STMDI^2) +
LODG + I(LODG^2) +
NOHAV + I(NOHAV^2) +
PPSTD + I(PPSTD^2),
data = model_data)
# Method 2: Using poly() function (produces orthogonal polynomials)
# This can help with multicollinearity issues
poly_model_orthogonal <- lm(FYLD ~ poly(NDVI, 2, raw = FALSE) +
poly(PLTHT, 2, raw = FALSE) +
poly(CHL, 2, raw = FALSE) +
poly(ANGBR, 2, raw = FALSE) +
poly(STMDI, 2, raw = FALSE) +
poly(LODG, 2, raw = FALSE) +
poly(NOHAV, 2, raw = FALSE) +
poly(PPSTD, 2, raw = FALSE),
data = model_data)
# Display model summary
cat("\n", paste(rep("=", 80), collapse = ""), "\n")
cat("POLYNOMIAL REGRESSION MODEL SUMMARY (with raw polynomials)\n")
cat(paste(rep("=", 80), collapse = ""), "\n")
print(summary(poly_model))
cat("\n", paste(rep("=", 80), collapse = ""), "\n")
cat("POLYNOMIAL REGRESSION MODEL SUMMARY (with orthogonal polynomials)\n")
cat(paste(rep("=", 80), collapse = ""), "\n")
print(summary(poly_model_orthogonal))
# Extract coefficients
cat("\n", paste(rep("=", 80), collapse = ""), "\n")
cat("MODEL COEFFICIENTS\n")
cat(paste(rep("=", 80), collapse = ""), "\n")
coefficients_df <- as.data.frame(coef(poly_model))
names(coefficients_df) <- "Coefficient"
print(coefficients_df)
# Calculate and display confidence intervals
cat("\n", paste(rep("=", 80), collapse = ""), "\n")
cat("CONFIDENCE INTERVALS (95%)\n")
cat(paste(rep("=", 80), collapse = ""), "\n")
conf_int <- confint(poly_model, level = 0.95)
print(conf_int)
# ANOVA table
cat("\n", paste(rep("=", 80), collapse = ""), "\n")
cat("ANOVA TABLE\n")
cat(paste(rep("=", 80), collapse = ""), "\n")
print(anova(poly_model))
# Model diagnostics
cat("\n", paste(rep("=", 80), collapse = ""), "\n")
cat("MODEL DIAGNOSTICS\n")
cat(paste(rep("=", 80), collapse = ""), "\n")
# R-squared and Adjusted R-squared
cat("R-squared:", summary(poly_model)$r.squared, "\n")
cat("Adjusted R-squared:", summary(poly_model)$adj.r.squared, "\n")
cat("F-statistic:", summary(poly_model)$fstatistic[1], "\n")
cat("p-value (F-test):", pf(summary(poly_model)$fstatistic[1],
summary(poly_model)$fstatistic[2],
summary(poly_model)$fstatistic[3],
lower.tail = FALSE), "\n")
# AIC and BIC
cat("AIC:", AIC(poly_model), "\n")
cat("BIC:", BIC(poly_model), "\n")
# Check for multicollinearity using Variance Inflation Factor (VIF)
# Install car package if needed
if (!require(car)) {
install.packages("car")
library(car)
}
cat("\n", paste(rep("=", 80), collapse = ""), "\n")
cat("VARIANCE INFLATION FACTOR (VIF) - check for multicollinearity\n")
cat(paste(rep("=", 80), collapse = ""), "\n")
tryCatch({
vif_values <- vif(poly_model)
print(vif_values)
}, error = function(e) {
cat("Could not calculate VIF due to: ", e$message, "\n")
})
# Create diagnostic plots
cat("\n", paste(rep("=", 80), collapse = ""), "\n")
cat("GENERATING DIAGNOSTIC PLOTS\n")
cat(paste(rep("=", 80), collapse = ""), "\n")
# Set up plotting area
par(mfrow = c(2, 2))
plot(poly_model)
par(mfrow = c(1, 1))
# Additional diagnostic plots
# Residuals vs Fitted with smoother
plot(poly_model$fitted.values, poly_model$residuals,
main = "Residuals vs Fitted Values",
xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
lines(lowess(poly_model$fitted.values, poly_model$residuals), col = "blue")
# Q-Q plot for normality of residuals
qqnorm(poly_model$residuals, main = "Q-Q Plot of Residuals")
qqline(poly_model$residuals, col = "red")
# Shapiro-Wilk test for normality
cat("\nShapiro-Wilk test for normality of residuals:\n")
shapiro_test <- shapiro.test(poly_model$residuals[1:5000])  # Test only first 5000 due to sample size limit
print(shapiro_test)
# Predicted vs Actual values plot
predicted <- predict(poly_model)
plot(model_data$FYLD, predicted,
main = "Predicted vs Actual FYLD",
xlab = "Actual FYLD", ylab = "Predicted FYLD")
abline(0, 1, col = "red", lty = 2)
# Calculate prediction accuracy metrics
rmse <- sqrt(mean((predicted - model_data$FYLD)^2))
mae <- mean(abs(predicted - model_data$FYLD))
cat("\nPrediction Accuracy Metrics:\n")
cat("RMSE:", rmse, "\n")
cat("MAE:", mae, "\n")
cat("Mean FYLD:", mean(model_data$FYLD), "\n")
cat("RMSE as % of mean:", (rmse/mean(model_data$FYLD))*100, "%\n")
# Create a summary table of important statistics
cat("\n", paste(rep("=", 80), collapse = ""), "\n")
cat("MODEL SUMMARY TABLE\n")
cat(paste(rep("=", 80), collapse = ""), "\n")
# Extract coefficients, standard errors, t-values, and p-values
coef_summary <- summary(poly_model)$coefficients
coef_table <- data.frame(
Estimate = coef_summary[, 1],
t_value = coef_summary[, 3],
Significance = ifelse(coef_summary[, 4] < 0.001, "***",
ifelse(coef_summary[, 4] < 0.01, "**",
ifelse(coef_summary[, 4] < 0.05, "*",
ifelse(coef_summary[, 4] < 0.1, ".", " "))))
)
# Print the coefficient table
print(coef_table)
